<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="bootstrap.min.css" rel="stylesheet" />
    <meta name="twitter:card" content="summary_large_image" />
    <!-- anon-on -->
    <meta name="twitter:site" content="@GoogleAI" />
    <!-- anon-off -->
    <meta name="twitter:title" content="Instruct-Imagen: Imagen Generation with Multi-modal Instruction" />
    <meta name="twitter:description" content="In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization." />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="styles.css" />

    <script src="jquery.min.js"></script>
    <script src="jquery.flip.min.js"></script>


    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SMWK19DNCN"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-SMWK19DNCN');
    </script>

    <title>Instruct-Imagen: Imagen Generation with Multi-modal Instruction</title>
  </head>
  <body>
    <div id="gallery0" style="padding-top: 50px; padding-bottom: 20px; background-color: rgb(245,245,245);"> 
      <h2 style="text-align: center;"><strong>Instruct-Imagen</strong>: Imagen Generation with Multi-modal Instruction</h2>
      
      <!-- anon-on -->
      <div class="authors text-center">
        <div class="row">
          <div class="col">
            <a href="https://www.hexianghu.com/">Hexiang Hu</a><sup>&#x2B25;*</sup>
          </div>
          <div class="col">
            <a href="https://ckkelvinchan.github.io/">Kelvin C.K. Chan</a><sup>&#x2B26;*</sup>
          </div>
          <div class="col">
            <a href="https://sammy-su.github.io/">Yu-Chuan Su</a><sup>&#x2B26;*</sup>
          </div>
          <div class="col">
            <a href="https://wenhuchen.github.io/">Wenhu Chen</a><sup>&#x2B25;*</sup>
          </div>
        </div>
        <div class="row">
          <div class="col">
            <a href="https://cold-winter.github.io/">Yandong Li</a><sup>&#x2B26;</sup>
          </div>
          <div class="col">
            <a href="https://sites.google.com/site/kihyuksml/">Kihyuk Sohn</a><sup>&#x2B26;</sup>
          </div>
          <div class="col">
            <a href="https://sites.google.com/corp/view/zhao-yang/">Yang Zhao</a><sup>&#x2B26;</sup>
          </div>
          <div class="col">
            <a href="https://scholar.google.com/citations?user=PZozDjoAAAAJ&hl=en">Xue Ben</a><sup>&#x2B26;</sup>
          </div>
          <div class="col">
            <a href="http://boqinggong.info/">Boqing Gong</a><sup>&#x2B26;</sup>
          </div>
        </div>
        <div class="row">
          <div class="col">
            <a href="http://www.cs.cmu.edu/~wcohen/">William W. Cohen</a><sup>&#x2B25;</sup>
          </div>
          <div class="col">
            <a href="https://mingweichang.org/">Ming-Wei Chang</a><sup>&#x2B25;</sup>
          </div>
          <div class="col">
            <a href="https://scholar.google.com/citations?user=vO0VSSYAAAAJ&hl=en">Xuhui Jia</a><sup>&#x2B26;</sup>
          </div>
        </div>
        <div class="row mt-1">
          <div class="col text-small">(*: Authors contributed equally)</div>
        </div>
        <div class="row mt-3">
          <div class="col">
            <span>&#x2B25;&nbsp;:&nbsp;</span>&nbsp;&nbsp;<img src="assets/gdm-logo.svg" alt="Google Deepmind" height=35>
          </div>
          <div class="col">
            <span>&#x2B26;&nbsp;:&nbsp;</span>&nbsp;&nbsp;<img src="assets/GoogleResearch.svg" alt="Google Research" height=50>
          </div>
        </div>
        <img src="assets/instruct-imagen-gif.gif" class="figure_image">
        <div class="row mt-3 nomobile">
          <center><a href="" target="_blank"><img class="thumbnail" src="assets/paper_thumbnail.png"></a></center>
        </div>
      </div>
      <!-- anon-off -->
    </div>

    <div class="header_dark_gray text-center">
      <h2>Instruct-Imagen <strong>generalizes</strong> to heterogeneous and complex image generation tasks.</h2>
    </div>

    <div class="abstract">
      <div class="inside">
        <p class="text">
          This paper presents Instruct-Imagen, a model that tackles heterogeneous image generation tasks and generalizes across unseen tasks. We introduce <strong>multi-modal instruction</strong> for image generation, a task representation articulating a range of generation intents with precision. It uses natural language to amalgamate disparate modalities (e.g., text, edge, style, subject, etc.), such that abundant generation intents can be standardized in a uniform format.<br><br> We then build the instruct-imagen by fine-tuning a pre-trained text-to-image diffusion model with a two-stage framework. First, we adapt the model using the retrieval-augmented training, to enhance model's capabilities to ground its generation on external multimodal context. Subsequently, we fine-tune the adapted model on diverse image generation tasks that requires vision-language understanding (e.g., subject-driven generation, etc.), each paired with a multi-modal instruction encapsulating the task's essence. Human evaluation on various image generation datasets reveals that instruct-imagen matches or surpasses prior task-specific models in-domain and demonstrates promising generalization to unseen and more complex tasks.
        </p>
        <!-- anon-on -->
        <a class="read-paper" href="" target="_blank"><button>Research Paper</button></a>
        <!-- anon-off -->
      </div>
    </div>


    <!-- anon-on -->
    <div class="abstract header_dark_gray">
      <div class="inside">
        <pre>
          <p>Cite us using the following BibTex:</p>
          <code>
          @article{hu2024instruct,
            title={Instruct-Imagen: Image Generation with Multi-modal Instruction}, 
            author={Hu, Hexiang and Chan, Kelvin and Su, Yu-Chuan and Chen, Wenhu 
                    and Li, Yandong and Yang, Zhao and Ben, Xue and Gong, Boqing 
                    and Cohen, William W and Chang, Ming-Wei and Jia, Xuhui},
            journal={arXiv preprint arXiv:240x.xxxx},
            year={2024},
          }
          </code>
        </pre>
      </div>
    </div>

    <div class="note text-center mt-5">
      <div class="inside">
        <h2>Border Impact</h2>
        <p class="text">Text-to-image generation models like Imagen and Stable Diffusion present ethical concerns, including social bias. Instruct-Imagen, using similar Web-scale datasets, faces these same issues. Instruct-Imagen's retrieval-augmented training and multi-modal instruction-tuning have notably enhanced image controllability and attribution. This control can be beneficial or harmful. A risk is using Instruct-Imagen for malicious activities, such as creating misleading images of people. Conversely, it offers advantages, like reducing image hallucination and improving relevance to user intent. It also benefits minority communities by effectively generating images of less-known landmarks, foods, and cultural artifacts, addressing the bias in AI systems. To mitigate public risks, we'll be cautious with code and API releases. Future work will focus on a responsible use framework, weighing the benefits of research transparency against the dangers of open access, ensuring safe and beneficial usage..</p>
      </div>
    </div>

    <div class="note header_dark_gray text-center">
      <div class="inside">
        <h2>Special Thanks</h2>
        <p class="text">We thank Zhiwei Deng, Jason Baldridge, Nando de Freitas for reviewing an early version of this paper in depth, with valuable comments and suggestions. Special thanks to Han Zhang for project idea discussion in the early stage of this project. We also thank Irina Blok for providing a style image used in our evaluation.</p>
      </div>
    </div>

    <!-- anon-off -->

    <script src="bootstrap.bundle.min.js"></script>
      </body>
    </html>